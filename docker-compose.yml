version: '3.7'

services:
  # Aplicação principal
  predictive-analytics:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8050:8050"
    volumes:
      - ./data:/app/data
    environment:
      - DATA_STORAGE_PATH=/app/data/sales_data.parquet
      

   # Apache Airflow Webserver
  airflow-webserver:
    image: apache/airflow:2.1.0
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://your_username:your_password@db:5432/airflow
    ports:
      - "8081:8080"
    volumes:
      - ./dags:/opt/airflow/dags
    depends_on:
      - db

  # Apache Airflow Scheduler
  airflow-scheduler:
    image: apache/airflow:2.1.0
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://your_username:your_password@db:5432/airflow
    volumes:
      - ./dags:/opt/airflow/dags
    depends_on:
      - db     
  
  # Apache Spark (Assumindo uso de Spark master e worker)
  spark-master:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=master
    ports:
      - "8080:8080"

  spark-worker:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G

  # Para SQL, vou supor um serviço PostgreSQL
  db:
    image: postgres:latest
    environment:
      - POSTGRES_USER=your_username
      - POSTGRES_PASSWORD=your_password
      - POSTGRES_DB=sales_data
    volumes:
      - pgdata:/var/lib/postgresql/data

volumes:
  pgdata:
